#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¥ HAKEEM Medical Bot v3.2 - FINAL FIX
- Fixed Groq Model (llama-3.3)
- Fixed Gemini API URL structure
- Auto-fallback enabled
- Single file production-ready
"""

import os
import sys
import sqlite3
import logging
import asyncio
from datetime import datetime
from pathlib import Path
import httpx
from dotenv import load_dotenv
from telegram import Update, ReplyKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.constants import ChatAction

# ============================================================================
# CONFIGURATION & LOGGING
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# API Endpoints
GROQ_URL = "https://api.groq.com/openai/v1/chat/completions"
# ØªÙ… ØªØµØ­ÙŠØ­ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù†Ø§ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù…ÙƒØªØ¨Ø© httpx
GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

# ============================================================================
# ENGINES
# ============================================================================

class MedicalEngine:
    async def get_response(self, query: str, bot, chat_id: int):
        # 1. Try Groq
        resp = await self._groq_call(query, bot, chat_id)
        if resp: return resp, "Groq"
        
        # 2. Fallback to Gemini
        logger.warning("Falling back to Gemini...")
        resp = await self._gemini_call(query, bot, chat_id)
        if resp: return resp, "Gemini"
        
        return None, None

    async def _groq_call(self, query, bot, chat_id):
        if not GROQ_API_KEY: return None
        try:
            await bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
            headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
            payload = {
                "model": "llama-3.3-70b-versatile",
                "messages": [
                    {"role": "system", "content": "Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…ØªØ®ØµØµ. Ù‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡ Ù„Ø¶Ø±ÙˆØ±Ø© Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨."},
                    {"role": "user", "content": query}
                ]
            }
            async with httpx.AsyncClient(timeout=30.0) as client:
                r = await client.post(GROQ_URL, json=payload, headers=headers)
                if r.status_code == 200:
                    return r.json()['choices'][0]['message']['content']
                logger.error(f"Groq Error {r.status_code}: {r.text}")
        except Exception as e: logger.error(f"Groq Exception: {e}")
        return None

    async def _gemini_call(self, query, bot, chat_id):
        if not GEMINI_API_KEY: return None
        try:
            await bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
            url = f"{GEMINI_BASE_URL}?key={GEMINI_API_KEY}"
            payload = {
                "contents": [{"parts": [{"text": query}]}],
                "systemInstruction": {"parts": [{"text": "Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ø®Ø¨ÙŠØ±. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."}]}
            }
            async with httpx.AsyncClient(timeout=30.0) as client:
                r = await client.post(url, json=payload)
                if r.status_code == 200:
                    return r.json()['candidates'][0]['content']['parts'][0]['text']
                logger.error(f"Gemini Error {r.status_code}: {r.text}")
        except Exception as e: logger.error(f"Gemini Exception: {e}")
        return None

engine = MedicalEngine()

# ============================================================================
# BOT LOGIC
# ============================================================================

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    kb = [["ğŸ’¬ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø©"], ["â“ Ù…Ø³Ø§Ø¹Ø¯Ø©"]]
    await update.message.reply_text(
        "ğŸ¥ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø­ÙƒÙŠÙ… Ø§Ù„Ø·Ø¨ÙŠ v3.2\nØ§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø¢Ù† ÙˆØ³Ø£Ø¬ÙŠØ¨Ùƒ ÙÙˆØ±Ø§Ù‹.",
        reply_markup=ReplyKeyboardMarkup(kb, resize_keyboard=True)
    )

async def handle_msg(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text
    user_id = update.effective_user.id
    
    print(f"\n[ğŸ“¨] Ø±Ø³Ø§Ù„Ø© Ù…Ù† {user_id}: {text}")
    
    wait_msg = await update.message.reply_text("ğŸ¤” Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø³Ø¤Ø§Ù„Ùƒ Ø·Ø¨ÙŠÙ‹Ø§...")
    
    response, used = await engine.get_response(text, context.bot, update.message.chat_id)
    
    await wait_msg.delete()
    
    if response:
        print(f"[âœ…] ØªÙ… Ø§Ù„Ø±Ø¯ Ø¨ÙˆØ§Ø³Ø·Ø© {used}")
        await update.message.reply_text(f"ğŸ¤– **Ø­ÙƒÙŠÙ… ({used}):**\n\n{response}", parse_mode="Markdown")
    else:
        print("[âŒ] ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª")
        await update.message.reply_text("âš ï¸ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ù…Ø´ØºÙˆÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. Ø­Ø§ÙˆÙ„ Ø«Ø§Ù†ÙŠØ©.")

# ============================================================================
# MAIN
# ============================================================================

def main():
    if not TELEGRAM_TOKEN:
        print("âŒ TELEGRAM_TOKEN missing in .env")
        return

    app = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # Ø§Ù„ØªØ±ØªÙŠØ¨ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_msg))
    
    print("ğŸš€ Ø­ÙƒÙŠÙ… v3.2 ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù†... Ø§Ø¨Ø¹Ø« Ø±Ø³Ø§Ù„Ø© Ù„ØªØ¬Ø±Ø¨ØªÙ‡!")
    app.run_polling()

if __name__ == "__main__":
    main()
